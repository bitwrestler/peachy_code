syntax="proto3";

//Server implementation that starts the model and listens for requests 
service PeachyServer {
    //Submit a request (prompt) to the server
    rpc Submit (DiffRequest) returns (DiffResult);
    //Return the currently running nvidia-smi stats
    rpc GPUStats(DiffRequest) returns (DiffResult);
    //Request shutdown the server
    rpc Shutdown(Empty) returns (Empty);
    //Set server settings
    rpc ChangeSettings(Settings) returns (Empty);
}

//The type of prompt being system or user
enum PromptType {
    PromptType_USER = 0;
    PromptType_SYSTEM = 1;
}

//The type of response being completed or queued
enum ResponseType {
    ResponseType_COMPLETE = 0;
    ResponseType_QUEUED = 1;
}

//The Prompt item (prompt type and string value)
message PromptItem {
    //Prompt Type
    PromptType Type=1;
    //string prompt
    string Prompt=2;
}

//The Request message
message DiffRequest {
    //Request prompt string
    repeated PromptItem Request=1;
    //The unique result id returned by DiffResult in order to check the status of a queued request
    string ResultID=2;
}

//The Response message
message DiffResult {
    //String array result
    repeated string Result=1;
    //Enum indicating the type of response
    ResponseType ResultType=2;
    //The unique result id for this request, can be used in the case of a queued request to check its status
    string ResultID=3;
}

//Settings message for changing the server settings from the client
message Settings {
    //float value to set the temperature of the server
    float Temperature=1;
}

message Empty {}